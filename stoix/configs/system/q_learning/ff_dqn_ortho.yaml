# --- DQN with Gram Orthogonalization Regularization ---

system_name: ff_dqn_ortho  # Name of the system.

# === Orthogonalization Settings ===
# ortho_mode: "loss" (default) - apply ortho via loss function: loss += ortho_lambda * gram_loss
#             "optimizer" - apply ortho via AdamO: decoupled from loss, applied in optimizer
ortho_mode: loss              # "loss" or "optimizer" (AdamO)
ortho_lambda: 0.2             # Gram regularization strength for loss mode (0.2 is recommended)
ortho_coeff: 1e-3             # Ortho coefficient for optimizer mode (AdamO)
ortho_exclude_output: true    # Don't constrain final layer (for both modes)
log_spectral_freq: 1          # Log SVD diagnostics every N evaluations (set higher for long runs)

# === Standard DQN Settings ===
rollout_length: 2             # Number of environment steps per vectorised environment.
epochs: 16                    # Number of sgd steps per rollout.
warmup_steps: 16              # Number of steps to collect before training.
total_buffer_size: 1_000_000  # Total effective size of the replay buffer.
total_batch_size: 512         # Total effective number of samples to train on.
q_lr: 5e-4                    # Learning rate of the Q network optimizer.
tau: 0.005                    # Smoothing coefficient for target networks.
gamma: 0.99                   # Discount factor.
max_grad_norm: 0.5            # Maximum norm of the gradients for a weight update.
decay_learning_rates: False   # Whether learning rates should be linearly decayed.
training_epsilon: 0.1         # Epsilon for epsilon-greedy policy during training.
evaluation_epsilon: 0.00      # Epsilon for epsilon-greedy policy during evaluation.
max_abs_reward: 1000.0        # Maximum absolute reward value.
huber_loss_parameter: 0.0     # Parameter for Huber loss. If 0, uses MSE loss.
